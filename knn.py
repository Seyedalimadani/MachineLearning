# -*- coding: utf-8 -*-
"""KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NjXXzfqF5O_PYNaPuK7M-WVfJj5CBbs6
"""

import sklearn
from sklearn.utils import shuffle
from sklearn.neighbors import KNeighborsClassifier
import pandas as pd
import numpy as np
from sklearn import linear_model, preprocessing

""""preprocessing" will be used to normalize our data and convert non-numeric values into numeric values."""

data = pd.read_csv("KNNData.data")
print(data.head())  # To check if our data is loaded correctly

"""much of our data is not numeric. In order to train the K-Nearest Neighbor Classifier we must convert any string data into some kind of a number. Luckily for us sklearn has a method that can do this for us.

We will start by creating a label encoder object and then use that to encode each column of our data into integers.
"""

le = preprocessing.LabelEncoder()

"""The method fit_transform() takes a list (each of our columns) and will return to us an array containing our new values."""

buying = le.fit_transform(list(data["buying"]))
maint = le.fit_transform(list(data["maint"]))
door = le.fit_transform(list(data["door"]))
persons = le.fit_transform(list(data["persons"]))
lug_boot = le.fit_transform(list(data["lug_boot"]))
safety = le.fit_transform(list(data["safety"]))
cls = le.fit_transform(list(data["class"]))

"""Now we need to recombine our data into a feature list and a label list. We can use the zip() function to makes things easier."""

X = list(zip(buying, maint, door, persons, lug_boot, safety))  # features
y = list(cls)  # labels

"""we will split our data into training and testing data"""

x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size = 0.1)

"""KNN Classifier is almost identical to how we created the linear regression model. The only difference is we can specify how many neighbors to look for as the argument n_neighbors."""

model = KNeighborsClassifier(n_neighbors=9)

"""train our model"""

model.fit(x_train, y_train)

"""to score our model we will do the following."""

acc = model.score(x_test, y_test)
print(acc)

"""If we'd like to see how our model is performing on the unique elements of our test data we can do the following."""

predicted = model.predict(x_test)
names = ["unacc", "acc", "good", "vgood"]

for x in range(len(predicted)):
    print("Predicted: ", names[predicted[x]], "Data: ", x_test[x], "Actual: ", names[y_test[x]])

# This will display the predicted class, our data and the actual class
# We create a names list so that we can convert our integer predictions into 
# their string representation

"""The KNN model has a unique method that allows for us to see the neighbors of a given data point. We can use this information to plot our data and get a better idea of where our model may lack accuracy. We can use model.neighbors to do this."""

predicted = model.predict(x_test)
names = ["unacc", "acc", "good", "vgood"]

for x in range(len(predicted)):
    print("Predicted: ", names[predicted[x]], "Data: ", x_test[x], "Actual: ", names[y_test[x]])
    # Now we will we see the neighbors of each point in our testing data
    n = model.kneighbors([x_test[x]], 9, True)
    print("N: ", n)